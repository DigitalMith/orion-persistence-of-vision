# ========================================
# âœ… Orion Project - config.yaml
# For use with llama.cpp and RTX 3060
# ========================================

# Map model name patterns to model_type
.*(llama|alpac|vicuna|guanaco|koala|llava|wizardlm|metharme|pygmalion-7b|pygmalion-2|mythalion|wizard-mega|openbuddy|vigogne|h2ogpt-research|manticore):
  model_type: 'llama'

.*(opt-|opt_|opt1|opt3|optfor|galactica|galpaca|pygmalion-350m):
  model_type: 'opt'

.*(gpt-j|gptj|gpt4all-j|malion-6b|pygway|pygmalion-6b|dolly-v1):
  model_type: 'gptj'

.*(gpt-neox|koalpaca-polyglot|polyglot.*koalpaca|polyglot-ko|polyglot_ko|pythia|stablelm|incite|dolly-v2|polycoder|h2ogpt-oig|h2ogpt-oasst1|h2ogpt-gm):
  model_type: 'gptneox'

.*bloom:
  model_type: 'bloom'

.*gpt2:
  model_type: 'gpt2'

.*falcon:
  model_type: 'falcon'

.*mpt:
  model_type: 'mpt'

# Specific settings for llama.cpp backend
llama_cpp:
  n_gpu_layers: 100     # Offload up to 100 layers to GPU (safe for RTX 3060 12GB)
  main_gpu: 0           # Use primary GPU (default)
  use_mlock: true       # Prevent memory swap for better performance
  use_mmap: true        # Enable memory mapping (faster startup/load)
  low_vram: false       # Let it use full GPU potential (set to true for GPUs < 8GB)
  numa: false           # Only enable for NUMA systems (not typical)

# Optional: If using multiple models/loaders
# You can extend with other loaders like exllama, transformers, etc.
